{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b4271",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain-xai langchain python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5cb8ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bfa2c1",
   "metadata": {},
   "source": [
    "#### 1. Zero-Shot Prompting\n",
    "\n",
    "- Description: Directly asking the model to perform a task without providing examples. The prompt relies on the model’s pre-trained knowledge to generate a response.\n",
    "\n",
    "- Use Case: Quick tasks where the model is expected to understand the instruction without further context.\n",
    "\n",
    "- Explanation: The prompt directly asks for the answer without examples, relying on the model’s knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd63e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_xai import ChatXAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"XAI_API_KEY\")\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatXAI(model_name=\"grok-3\", xai_api_key=api_key)\n",
    "\n",
    "# Define the zero-shot prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question: {question}\"\n",
    ")\n",
    "\n",
    "# Create a chain using RunnableSequence\n",
    "chain = prompt | llm\n",
    "\n",
    "# Run the chain\n",
    "question = \"What is the capital of France?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "# Handle the response (assuming ChatXAI returns a ChatMessage-like object)\n",
    "print(f\"Prompt: {question}\")\n",
    "print(f\"Response: {response.content if hasattr(response, \"content\") else response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffffe2cc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e845de8",
   "metadata": {},
   "source": [
    "#### 2. Few-Shot Prompting\n",
    "\n",
    "- Description: Providing a few examples within the prompt to guide the model’s response format or style. This helps the model understand the task better.\n",
    "\n",
    "- Use Case: Tasks requiring specific formats or styles, like classification or translation.\n",
    "\n",
    "- Explanation: The prompt includes examples to show the model how to classify sentiment, guiding it to produce a consistent response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_xai import ChatXAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"XAI_API_KEY\")\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatXAI(model_name=\"grok-3\", xai_api_key=api_key)\n",
    "\n",
    "# Define the few-shot prompt with examples\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Classify the sentiment of the following text as positive, negative, or neutral:\\n\\n\"\n",
    "             \"Example 1: 'I love this product!' -> Positive\\n\"\n",
    "             \"Example 2: 'This is disappointing.' -> Negative\\n\"\n",
    "             \"Example 3: 'The item is okay.' -> Neutral\\n\\n\"\n",
    "             \"Text: {text}\\nSentiment:\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "text = \"I'm really impressed with the service!\"\n",
    "response = chain.invoke({\"text\": text})\n",
    "print(f\"Prompt: {text}\")\n",
    "print(response.content if hasattr(response, \"content\") else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1cc96d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcfa744",
   "metadata": {},
   "source": [
    "#### 3. Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "- Description: Encouraging the model to reason step-by-step to arrive at an answer, often improving performance on complex tasks like math or logic.\n",
    "\n",
    "- Use Case: Arithmetic, logical reasoning, or multi-step problem-solving.\n",
    "\n",
    "- Explanation: The prompt explicitly asks for step-by-step reasoning, which helps the model produce a clear and accurate solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dae3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_xai import ChatXAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"XAI_API_KEY\")\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatXAI(model_name=\"grok-3\", xai_api_key=api_key)\n",
    "\n",
    "# Define the CoT prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Solve the following problem step-by-step:\\n{question}\\n\\nProvide your reasoning and final answer.\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "question = \"What is 15 × 8?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "print(f\"Prompt: {question}\")\n",
    "print(response.content if hasattr(response, \"content\") else response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680774a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16924425",
   "metadata": {},
   "source": [
    "#### 4. Self-Consistency Prompting\n",
    "\n",
    "- Description: Generating multiple responses to the same prompt and selecting the most consistent answer (e.g., via majority voting). This reduces errors from variability.\n",
    "\n",
    "- Use Case: Tasks where the model might produce inconsistent answers, like math or ambiguous questions.\n",
    "\n",
    "- Explanation: The example generates five responses and uses Counter to select the most frequent one. The temperature=0.7 encourages slight variations in responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dcae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_xai import ChatXAI\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"XAI_API_KEY\")\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatXAI(model_name=\"grok-3\", xai_api_key=api_key, temperature=0.7)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Solve the following problem: {question}\"\n",
    ")\n",
    "\n",
    "# Create a chain using RunnableSequence\n",
    "chain = prompt | llm\n",
    "\n",
    "# Generate multiple responses\n",
    "question = \"What is 15 × 8?\"\n",
    "responses = [chain.invoke({\"question\": question}) for _ in range(5)]\n",
    "\n",
    "# Extract content from responses (handle string or ChatMessage-like objects)\n",
    "response_contents = [\n",
    "    resp.content if hasattr(resp, \"content\") else str(resp) for resp in responses\n",
    "]\n",
    "\n",
    "# Normalize responses for majority voting (e.g., extract the number \"120\")\n",
    "normalized_responses = []\n",
    "for content in response_contents:\n",
    "    # Extract the numeric answer (assuming responses contain \"120\" or similar)\n",
    "    # Adjust this logic based on ChatXAI's output format\n",
    "    import re\n",
    "    numbers = re.findall(r'\\b\\d+\\b', content)\n",
    "    normalized_responses.append(numbers[0] if numbers else content.strip())\n",
    "\n",
    "# Aggregate results using majority voting\n",
    "answer_counts = Counter(normalized_responses)\n",
    "final_answer = answer_counts.most_common(1)[0][0]\n",
    "\n",
    "# Print responses and final answer\n",
    "print(f\"Prompt: {question}\")\n",
    "print(f\"Responses: {response_contents}\")\n",
    "print(f\"Normalized Responses: {normalized_responses}\")\n",
    "print(f\"Final Answer: {final_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6c9e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616283ea",
   "metadata": {},
   "source": [
    "#### 5. Role-Based Prompting\n",
    "\n",
    "- Description: Assigning a specific role or persona to the model (e.g., “You are a teacher”) to tailor the tone or perspective of the response.\n",
    "\n",
    "- Use Case: Responses requiring a specific tone, expertise, or style, like explaining concepts to a beginner.\n",
    "\n",
    "- Explanation: The prompt assigns the model a teacher role, ensuring a simple and engaging explanation tailored for students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_xai import ChatXAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"XAI_API_KEY\")\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatXAI(model_name=\"grok-3\", xai_api_key=api_key)\n",
    "\n",
    "# Define the role-based prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"You are a high school science teacher. Explain {topic} in a simple and engaging way for students.\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "topic = \"photosynthesis\"\n",
    "response = chain.invoke({\"topic\": topic})\n",
    "print(f\"Prompt: {topic}\")\n",
    "print(f\"Response: {response.content if hasattr(response, \"content\") else response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ceeed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf7028",
   "metadata": {},
   "source": [
    "#### 6. Prompt Chaining\n",
    "\n",
    "- Description: Breaking a complex task into multiple sequential prompts, where the output of one prompt feeds into the next. This is ideal for multi-step tasks.\n",
    "\n",
    "- Use Case: Tasks requiring multiple stages, like summarizing then analyzing a text.\n",
    "\n",
    "- Explanation: The first chain summarizes the text, and the second chain analyzes the sentiment of the summary, demonstrating how outputs can feed into subsequent prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c543f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_xai import ChatXAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"XAI_API_KEY\")\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatXAI(model_name=\"grok-3\", xai_api_key=api_key)\n",
    "\n",
    "# First prompt: Summarize text\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Summarize the following text in one sentence: {text}\"\n",
    ")\n",
    "chain1 = prompt1 | llm\n",
    "\n",
    "# Second prompt: Analyze sentiment\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"summary\"],\n",
    "    template=\"What is the sentiment of this summary? {summary}\"\n",
    ")\n",
    "chain2 = prompt2 | llm\n",
    "\n",
    "# Combine into a sequential chain\n",
    "def create_sequential_chain():\n",
    "    def process_summary(summary):\n",
    "        # Handle ChatXAI output (ChatMessage-like or string)\n",
    "        summary_text = summary.content if hasattr(summary, \"content\") else str(summary)\n",
    "        return {\"summary\": summary_text}\n",
    "\n",
    "    # Chain: summarize text, process output, then analyze sentiment\n",
    "    overall_chain = chain1 | process_summary | chain2\n",
    "    return overall_chain\n",
    "\n",
    "overall_chain = create_sequential_chain()\n",
    "\n",
    "# Run the chain\n",
    "text = \"The new park in town is beautiful, with vibrant flowers and clean pathways, but the parking is limited.\"\n",
    "response = overall_chain.invoke({\"text\": text})\n",
    "\n",
    "# Extract final response\n",
    "response_content = response.content if hasattr(response, \"content\") else str(response)\n",
    "\n",
    "# Print results with verbose-like output\n",
    "print(f\"Input Text: {text}\")\n",
    "print(f\"Final Response (Sentiment): {response_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3080fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c73b29",
   "metadata": {},
   "source": [
    "#### 7. Instruction-Based Prompting\n",
    "\n",
    "- Description: Providing explicit instructions within the prompt to guide the model’s behavior, such as specifying format, tone, or constraints.\n",
    "\n",
    "- Use Case: Tasks requiring structured output, like JSON or a specific tone.\n",
    "\n",
    "- Explanation: The prompt specifies the format (three bullet points) and tone (formal), ensuring the model adheres to these constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07afc36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_xai import ChatXAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"XAI_API_KEY\")\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatXAI(model_name=\"grok-3\", xai_api_key=api_key)\n",
    "\n",
    "# Define the instruction-based prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"Describe the product in exactly three bullet points, using a formal tone.\\nProduct: {product}\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "product = \"wireless earbuds\"\n",
    "response = chain.invoke({\"product\": product})\n",
    "print(f\"Prompt: {product}\")\n",
    "print(f\"Response: {response.content if hasattr(response, \"content\") else response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fe7773",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb4defb",
   "metadata": {},
   "source": [
    "#### 8. Contextual Prompting\n",
    "\n",
    "- Description: Including relevant context or background information in the prompt to improve the model’s understanding and response accuracy.\n",
    "\n",
    "- Use Case: Tasks requiring domain-specific knowledge or context, like answering questions about a specific document.\n",
    "\n",
    "- Explanation: The prompt provides specific context about the Eiffel Tower, enabling the model to give a precise answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd84613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_xai import ChatXAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"XAI_API_KEY\")\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatXAI(model_name=\"grok-3\", xai_api_key=api_key)\n",
    "\n",
    "# Define the contextual prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"Given the following context:\\n{context}\\n\\nAnswer the question: {question}\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "context = \"The Eiffel Tower, located in Paris, France, was completed in 1889 and stands 324 meters tall.\"\n",
    "question = \"What is the height of the Eiffel Tower?\"\n",
    "response = chain.invoke({\"context\": context, \"question\": question})\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Response: {response.content if hasattr(response, \"content\") else response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
